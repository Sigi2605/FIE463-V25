{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Last week\n",
    "\n",
    "-   Linear models:\n",
    "\n",
    "    -   Relationship between $y$ and $\\mathbf{x}$ given by:\n",
    "        $$\n",
    "            y_i = \\mu + \\mathbf{x}_i'\\bm{\\beta} + \\epsilon_i\n",
    "        $$\n",
    "\n",
    "    -   Estimated by minimizing **loss function** (OLS):\n",
    "        $$\n",
    "        L(\\mu, \\bm{\\beta}) = \n",
    "            \\underbrace{\\sum_{i=1}^N \\Bigl(\n",
    "            y_i - \\mu - \\mathbf{x}_i'\\bm{\\beta}\\Bigr)^2}_{\\text{Sum of squared errors}}\n",
    "        $$\n",
    "-   Creating additional features using polynomials\n",
    "-   Cross-validation to determine hyperparameters (polynomial degree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# This week\n",
    "\n",
    "1.  Linear regression models with **regularization**:\n",
    "\n",
    "    -   Ridge regression\n",
    "    -   Lasso\n",
    "    -   Elastic net (not covered, but trivial combination of Ridge & Lasso)\n",
    "\n",
    "2.  Models for **classification**:\n",
    "\n",
    "    -   Logistic regression\n",
    "    -   Random forest\n",
    "    -   Decision trees (in lecture notes)\n",
    "    -   Support Vector Machines (in lecture notes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Linear regression models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Ridge regression\n",
    "\n",
    "-   Loss function:\n",
    "    $$\n",
    "    L(\\mu, \\bm{\\beta}) = \n",
    "        \\underbrace{\\sum_{i=1}^N \\Bigl(\n",
    "        y_i - \\mu - \\mathbf{x}_i'\\bm{\\beta}\\Bigr)^2}_{\\text{Sum of squared errors}}\n",
    "        + \n",
    "        \\underbrace{\\alpha \\sum_{k=1}^K\\beta_k^2}_{\\text{L2 penalty}}\n",
    "    $$\n",
    "\n",
    "-   Penalty term introduces **shrinkage** or **regularization**\n",
    "\n",
    "    -   Large coefficients $\\beta_k$ are penalized\n",
    "\n",
    "-   Resulting model is biased, but has lower variance when making predictions\n",
    "    on new data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Example: Polynomial approximation\n",
    "\n",
    "-   True relationship given by trigonometric function, measured\n",
    "    with error $\\epsilon_i$:\n",
    "    $$\n",
    "    \\begin{aligned}\n",
    "    y_i &= \\cos\\left( \\frac{3}{2}\\pi x_i \\right) + \\epsilon_i \\\\\n",
    "        \\epsilon_i &\\stackrel{\\text{iid}}{\\sim} \\mathcal{N}\\left(0, 0.5^2\\right)\n",
    "    \\end{aligned}\n",
    "    $$\n",
    "\n",
    "-   Want to approximate this function with polynomials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Create sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable automatic reloading of external modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lecture12_regression import create_trig_sample\n",
    "\n",
    "# Sample size\n",
    "N = 200\n",
    "\n",
    "# Standard deviation of error term\n",
    "sigma = 0.5\n",
    "\n",
    "# Create sample data for trigonometric relationship between x and y\n",
    "x, y = create_trig_sample(N=N, sigma=sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Visualize sample and true function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lecture12_regression import plot_trig_sample\n",
    "\n",
    "plot_trig_sample(x, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Estimate Ridge regression\n",
    "\n",
    "1.  Assume function is **approximated** by polynomial of degree $K$:\n",
    "\n",
    "    $$\n",
    "    y_i \\approx \\mu + \\beta_1 x_i + \\beta_2 x_i^2 + \\cdots + \\beta_K x_i^K \n",
    "    $$\n",
    "\n",
    "    -   Set $K=15$ for illustration\n",
    "\n",
    "2.  Create pipeline ([`Pipeline`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) or [`make_pipeline()`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html)):\n",
    "\n",
    "    1.  Feature transformation: [`PolynomialFeatures`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html)\n",
    "    2.  Feature standardization: [`StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)\n",
    "    3.  Estimation: [`Ridge`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html)\n",
    "        -   Use regularization strength $\\alpha = 3$\n",
    "       \n",
    "3.  Estimate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max. polynomial degree\n",
    "degree = 15\n",
    "\n",
    "# TODO: Create pipeline with alpha = 3\n",
    "# pipe_ridge = \n",
    "\n",
    "# TODO: Fit ridge regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Estimate linear regression\n",
    "\n",
    "-   Useful as benchmark model\n",
    "-   Do we need feature standardization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create pipeline with linear regression\n",
    "# pipe_lr = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Plot predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Values at which to predict\n",
    "xvalues = np.linspace(0.0, 1.0, 100)\n",
    "\n",
    "# TODO: Compute predicted values from Ridge \n",
    "# y_pred_ridge = \n",
    "\n",
    "# TODO: Compute predicted values from linear regression\n",
    "# y_pred_lr ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot sample and true relationship\n",
    "ax = plot_trig_sample(x, y)\n",
    "\n",
    "# Linear regression prediction\n",
    "ax.plot(xvalues, y_pred_lr, c='purple', alpha=0.7, label='Linear regression')\n",
    "\n",
    "# Ridge prediction\n",
    "ax.plot(xvalues, y_pred_ridge, c='darkorange', lw=2.0, label='Ridge')\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Intuition: coefficients vs. regularization strength\n",
    "\n",
    "-   What happens to magnitude of estimated $\\bm\\beta$ as we vary regularization strength $\\alpha$?\n",
    "-   Fit many Ridge models for a grid of $\\alpha$, plot coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "\n",
    "# Create grid of alphas spaced uniformly in logs on [5e-3, 1000]\n",
    "alphas = np.logspace(start=np.log10(5.0e-3), stop=3, num=100)\n",
    "\n",
    "# Re-create pipeline w/o Ridge estimator, estimation step differs for each alpha\n",
    "transform = make_pipeline(\n",
    "    PolynomialFeatures(degree=degree, include_bias=False), StandardScaler()\n",
    ")\n",
    "\n",
    "# Create polynomial features\n",
    "X_trans = transform.fit_transform(x[:, None])\n",
    "\n",
    "# Array to store coefficients for all alphas\n",
    "coefs = np.empty((len(alphas), X_trans.shape[1]))\n",
    "\n",
    "# TODO: loop over alphas, fit Ridge for each alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot coefficient arrays against penalty strength\n",
    "plt.figure(figsize=(6,4))\n",
    "\n",
    "plt.plot(alphas, coefs, lw=1.0)\n",
    "\n",
    "plt.xscale('log', base=10)\n",
    "plt.axhline(0.0, ls='--', lw=0.75, c='black')\n",
    "plt.xlabel(r'Regularization strength $\\alpha$ (log scale)')\n",
    "plt.ylabel('Coefficient value')\n",
    "plt.title('Ridge coefficients as function of regularization strength')\n",
    "plt.legend([rf'$\\beta_{{{i}}}$' for i in range(degree)], ncols=5, loc='lower right')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Tuning the regularization parameter via cross-validation\n",
    "\n",
    "-   Regularization strength $\\alpha$ can be cross-validated with\n",
    "    [`RidgeCV`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html)\n",
    "-   Uses MSE to find optimal $\\alpha$\n",
    "-   Use argument `store_cv_results=True` to store MSE for all candidate $\\alpha$ (to plot validation curve)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Run Ridge CV\n",
    "\n",
    "-   `RidgeCV` does not support pipelines, transform features manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "# RidgeCV does not support pipelines, so we need to transform x before\n",
    "# cross-validation.\n",
    "transform = make_pipeline(\n",
    "    PolynomialFeatures(degree=degree, include_bias=False), StandardScaler()\n",
    ")\n",
    "\n",
    "# Create standardized polynomial features\n",
    "X_trans = transform.fit_transform(x[:, None])\n",
    "\n",
    "# Define grid of alphas on [1e-5, 5]\n",
    "N_alphas = 100\n",
    "alphas = np.logspace(start=np.log10(1.0e-5), stop=np.log10(5), num=N_alphas)\n",
    "\n",
    "# TODO: fit RidgeCV with alphas\n",
    "# rcv = \n",
    "\n",
    "# TODO: store and report best alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Plot validation curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute average MSE for each alpha\n",
    "# mse_mean = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot MSE against alphas, highlight minimum MSE\n",
    "plt.plot(alphas, mse_mean)\n",
    "plt.xlabel(r'Regularization strength $\\alpha$ (log scale)')\n",
    "plt.ylabel('Cross-validated MSE')\n",
    "plt.scatter(alphas[imin], mse_mean[imin], s=15, c='black', zorder=100)\n",
    "plt.axvline(alphas[imin], ls=':', lw=0.75, c='black')\n",
    "plt.xscale('log')\n",
    "plt.title('Validation curve for Ridge regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Re-estimate model with optimal alpha (optional)\n",
    "\n",
    "-   Not strictly need, could directly use fitted `RidgeCV` object\n",
    "-   But `RidgeCV` does not support pipelines..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create pipeline with optimal alpha\n",
    "pipe_ridge = make_pipeline(\n",
    "    PolynomialFeatures(degree=degree, include_bias=False),\n",
    "    StandardScaler(),\n",
    "    # TODO: Add Ridge estimator\n",
    ")\n",
    "\n",
    "# TODO: Fit Ridge with optimal alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Plot predictions from optimal Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid on which to evaluate predictions\n",
    "xvalues = np.linspace(np.amin(x), np.amax(x), 100)\n",
    "\n",
    "# TODO: Predicted values from Ridge regression\n",
    "# y_pred = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot sample and true relationship\n",
    "ax = plot_trig_sample(x, y)\n",
    "\n",
    "# Plot predicted values from cross-validated Ridge regression\n",
    "ax.plot(xvalues, y_pred, c='darkorange', lw=2.0, label=r'Ridge (optimal $\\alpha$)')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<h3> Your turn</h3>\n",
    "\n",
    "Rerun the whole Ridge example with a smaller sample size of <i>N=50</i>. What happens to the optimal cross-validated penalty parameter <i>ɑ</i>?\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Lasso\n",
    "\n",
    "-   Same idea as Ridge regression, but different penalty term:\n",
    "    $$\n",
    "    L(\\mu, \\bm{\\beta}) = \n",
    "        \\frac{1}{2N} \\underbrace{\\sum_{i=1}^N \\Bigl(\n",
    "        y_i - \\mu - \\mathbf{x}_i'\\bm{\\beta}\\Bigr)^2}_{\\text{Sum of squared errors}}\n",
    "        + \n",
    "        \\underbrace{\\alpha \\sum_{k=1}^K |\\beta_k|}_{\\text{L1 penalty}}\n",
    "    $$\n",
    "\n",
    "-   L1 penalty leads to **sparse models** with **fewer** nonzero coefficients"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Create sample\n",
    "\n",
    "-   Recreate same sample as in Ridge example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lecture12_regression import create_trig_sample\n",
    "\n",
    "# Sample size\n",
    "N = 200\n",
    "\n",
    "# Standard deviation of error term\n",
    "sigma = 0.5\n",
    "\n",
    "# Create sample data for trigonometric relationship between x and y\n",
    "x, y = create_trig_sample(N=N, sigma=sigma)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Estimate Lasso\n",
    "\n",
    "1.  Create pipeline ([`Pipeline`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) or [`make_pipeline()`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html)):\n",
    "\n",
    "    1.  Feature transformation: [`PolynomialFeatures`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html)\n",
    "    2.  Feature standardization: [`StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)\n",
    "    3.  Estimation: [`Lasso`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html)\n",
    "        -   Use regularization strength $\\alpha = 0.0075$\n",
    "        -   Might need to increase `max_iter` argument\n",
    "       \n",
    "2.  Estimate model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Polynomial degree\n",
    "degree = 15\n",
    "\n",
    "# TODO: Build pipeline of transformations and Lasso estimation.\n",
    "pipe_lasso = make_pipeline(\n",
    "    PolynomialFeatures(degree=degree, include_bias=False),\n",
    "    StandardScaler(),\n",
    "    # TODO: Add Lasso estimator\n",
    ")\n",
    "\n",
    "# TODO: Fit Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Plot predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid on which to evaluate predictions\n",
    "xvalues = np.linspace(np.amin(x), np.amax(x), 100)\n",
    "\n",
    "# TODO: Predicted values from Lasso regression\n",
    "# y_pred_lasso = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plot_trig_sample(x, y)\n",
    "\n",
    "# Linear regression prediction\n",
    "ax.plot(xvalues, y_pred_lr, c='purple', alpha=0.7, label='Linear regression')\n",
    "\n",
    "# Lasso prediction\n",
    "ax.plot(xvalues, y_pred_lasso, c='darkorange', lw=2.0, label='Lasso')\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Intuition: coefficients vs. regularization strength\n",
    "\n",
    "-   Use [`lasso_path()`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.lasso_path.html)\n",
    "    to compute coefficients for a grid of $\\alpha$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import lasso_path\n",
    "\n",
    "# Create grid of alphas spaced uniformly in logs\n",
    "alphas = np.logspace(start=np.log10(1.0e-3), stop=np.log10(1.0), num=100)\n",
    "\n",
    "# Re-create pipeline w/o Lasso estimator, estimation step differs for each alpha\n",
    "transform = make_pipeline(\n",
    "    PolynomialFeatures(\n",
    "        degree=degree, \n",
    "        include_bias=False\n",
    "    ),\n",
    "    StandardScaler()\n",
    ")\n",
    "\n",
    "# Create polynomial features\n",
    "X_trans = transform.fit_transform(x[:, None])\n",
    "\n",
    "# Compute Lasso path\n",
    "alphas, coefs, _ = lasso_path(X_trans, y, alphas=alphas, max_iter=100_000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Plot number of nonzero coefficients on $\\alpha$ grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of non-zero coefficients for each alpha. \n",
    "nonzero = np.sum(np.abs(coefs) > 1.0e-6, axis=0).astype(int)\n",
    "\n",
    "# Plot number of non-zero coefficients against alpha\n",
    "plt.plot(alphas, nonzero, lw=1.5, c='steelblue')\n",
    "plt.xscale('log', base=10)\n",
    "plt.yticks(np.arange(0, np.amax(nonzero) + 1))\n",
    "plt.xlabel(r'Regularization strength $\\alpha$ (log scale)')\n",
    "plt.title('Number of non-zero coefficients')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Plot coefficient magnitudes on $\\alpha$ grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(alphas, coefs.T, lw=1.0)\n",
    "plt.xscale('log', base=10)\n",
    "plt.axhline(0.0, ls='--', lw=0.75, c='black')\n",
    "plt.xlabel(r'Regularization strength $\\alpha$ (log scale)')\n",
    "plt.ylabel('Coefficient value')\n",
    "plt.title('Lasso coefficients as function of regularization strength')\n",
    "plt.legend([rf'$\\beta_{{{i}}}$' for i in range(degree)], ncols=5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Tuning the regularization parameter via cross-validation\n",
    "\n",
    "-   Regularization strength $\\alpha$ can be cross-validated with\n",
    "    [`LassoCV`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html)\n",
    "-   Uses MSE (or some other metric) to find optimal $\\alpha$\n",
    "-   Instead of $\\alpha$ grid we can specify $\\epsilon = \\frac{\\alpha_{min}}{\\alpha_{max}}$ (default: $10^{-3}$)\n",
    "    and the grid size "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Run Lasso CV\n",
    "\n",
    "-   `LassoCV` does not support pipelines, transform features manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "# LassoCV does not support pipelines, so we need to transform x before\n",
    "# cross-validation.\n",
    "transform = make_pipeline(\n",
    "    PolynomialFeatures(\n",
    "        degree=degree, \n",
    "        include_bias=False\n",
    "    ),\n",
    "    StandardScaler()\n",
    ")\n",
    "\n",
    "# Create standardized polynomial features\n",
    "X_trans = transform.fit_transform(x[:, None])\n",
    "\n",
    "# TODO: Create and run Lasso cross-validation, use defaults for eps and n_alphas\n",
    "# lcv = \n",
    "\n",
    "# TODO: Store and report best alpha\n",
    "# alpha_best =\n",
    "\n",
    "# TODO: Report number of non-zero coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Plot validation curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute average MSE for each alpha\n",
    "# mse_mean = \n",
    "\n",
    "# TODO: Compute index of minimal MSE\n",
    "# imin = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Recover grid of alphas used for CV\n",
    "alphas = lcv.alphas_\n",
    "\n",
    "# Plot MSE against alphas, highlight minimum MSE\n",
    "plt.plot(alphas, mse_mean)\n",
    "plt.xlabel(r'Regularization strength $\\alpha$ (log scale)')\n",
    "plt.ylabel('Cross-validated MSE')\n",
    "plt.scatter(alphas[imin], mse_mean[imin], s=15, c='black', zorder=100)\n",
    "plt.axvline(alphas[imin], ls=':', lw=0.75, c='black')\n",
    "plt.xscale('log')\n",
    "plt.title('Validation curve for Lasso')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Re-estimate model with optimal alpha (optional)\n",
    "\n",
    "-   Not strictly need, could directly use fitted `LassoCV` object\n",
    "-   But `LassoCV` does not support pipelines..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create pipeline with Lasso using optimal alpha\n",
    "pipe_lasso = make_pipeline(\n",
    "    PolynomialFeatures(\n",
    "        degree=degree, \n",
    "        include_bias=False\n",
    "    ),\n",
    "    StandardScaler(),\n",
    "    # TODO: Add Lasso estimator with optimal alpha\n",
    ")\n",
    "\n",
    "# TODO: Fit Lasso with optimal alpha"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Plot predicted values from optimal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid on which to evaluate predictions\n",
    "xvalues = np.linspace(np.amin(x), np.amax(x), 100)\n",
    "\n",
    "# TODO: Predicted values from Lasso regression\n",
    "# y_pred = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot sample and true relationship\n",
    "ax = plot_trig_sample(x, y)\n",
    "\n",
    "# Plot prediction from optimal Lasso model\n",
    "plt.plot(xvalues, y_pred, c='darkorange', lw=2.0, label=r'Lasso (optimal $\\alpha$)')\n",
    "    \n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Models for classification\n",
    "\n",
    "-   Predict categorical outcome (class label) instead of continuous outcome\n",
    "\n",
    "## Logistic regression\n",
    "\n",
    "-   Most simple setup: **binary** classifier with $y_i \\in \\{0, 1\\}$\n",
    "-   Probability of $y_i = 1$ is given by **sigmoid** function (logistic CDF):  \n",
    "    $$\n",
    "    p(\\mathbf{x}_i) \\equiv \n",
    "    \\text{Prob}\\bigl(y_i = 1 ~|~\\mathbf{x}_i\\bigr) \n",
    "        = \\frac{1}{1 + \\exp\\left(\\mu + \\mathbf{x}_i'\\bm\\beta\\right)}\n",
    "    $$\n",
    "-   Sigmoid function maps any real $z = \\mu + \\mathbf{x}_i'\\bm\\beta$ into $(0, 1)$:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import logistic\n",
    "\n",
    "zvalues = np.linspace(-6, 6, 50)\n",
    "\n",
    "plt.plot(zvalues, logistic.cdf(zvalues), lw=2.0)\n",
    "# Add horizontal and vertical lines\n",
    "for y in (0.0, 0.5, 1.0):\n",
    "    plt.axhline(y, ls='--', lw=0.75, c='black')\n",
    "plt.axvline(0.0, ls='--', lw=0.75, c='black')\n",
    "plt.xlabel('$z$')\n",
    "plt.ylabel(r'$\\sigma(z)$')\n",
    "plt.yticks([0.0, 0.5, 1.0])\n",
    "plt.title('Sigmoid function (logistic CDF)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   **Loss function:** derived from log likelihood (MLE) + penalty\n",
    "    $$\n",
    "    L(\\mu, \\bm\\beta) = \n",
    "    - \\underbrace{\\frac{1}{N} \\mathcal{L}(\\mu,\\bm\\beta)}_{\\text{scaled log likelihood}} \n",
    "    + \\underbrace{\\frac{r(\\bm\\beta)}{C}}_{\\text{regularization}}\n",
    "    $$\n",
    "\n",
    "    -   Regularization term $r(\\bm\\beta)$: L1, L2, L1 & L2, None\n",
    "    -   Regularization strength governed by $C$: large $C$ $\\Rightarrow$ small penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Example: Predicting binary class membership\n",
    "\n",
    "-   Stylized example: $y_i$ is a function of two features $(x_{1i}, x_{2i})$:\n",
    "    $$\n",
    "    \\begin{aligned}\n",
    "    y_i &= \n",
    "    \\begin{cases}\n",
    "        1 & \\text{if }~ f(x_{1i}, x_{2i}) + \\epsilon_i \\geq 0 \\\\\n",
    "        0 & \\text{else} \n",
    "    \\end{cases} \\\\\n",
    "    f(x_{1i}, x_{2i}) &= \\sin(2\\pi x_{1i}) \\cos(\\pi x_{2i}) \\\\\n",
    "    \\epsilon_i &\\stackrel{\\text{iid}}{\\sim} \\mathcal{N}\\left(0, \\sigma_{\\epsilon}^2\\right)\n",
    "    \\end{aligned}\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Create sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lecture12_classifiers import create_class_data\n",
    "\n",
    "# Sample size\n",
    "N = 100\n",
    "\n",
    "# Standard deviation of noise\n",
    "sigma_eps = 0.2\n",
    "\n",
    "# Create demo data set for classification\n",
    "X, y = create_class_data(N=N, sigma=sigma_eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lecture12_classifiers import plot_classes\n",
    "\n",
    "# Plot sample\n",
    "plot_classes(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Train-test split\n",
    "\n",
    "-   Use stratification to preserve relative frequency of class labels in training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# TODO: Split data into training and test sets\n",
    "# X_train, X_test, y_train, y_test = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Estimate logistic regression model (no regularization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Estimate simplest model with two features:\n",
    "    $$\n",
    "    z_i = \\mu + \\beta_1 x_{1i} + \\beta_2 x_{2i}\n",
    "    $$\n",
    "\n",
    "-   Implemented in [`LogisticRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
    "-   Relevant arguments:\n",
    "\n",
    "    -   `penalty`: Type of regularization to use\n",
    "    -   `C`: Regularization strength (ignored if `penalty=None`)\n",
    "    -   `max_iter`: may need to increase this from default value\n",
    "    -   `solver`: select solver (depends on `penalty`)\n",
    "    -   `random_state`: needed for some solvers which use RNG\n",
    "\n",
    "-   `LogisticRegression` implements four different types of regularization:\n",
    "\n",
    "    | Penalty | $r(\\bm\\beta)$                          | `penalty` argument |\n",
    "    |---------------------|----------------------------|---------------------|\n",
    "    | L1   | $r(\\bm\\beta) = \\|\\bm\\beta\\|_1 = \\sum_{k=1}^K \\|\\beta_k\\| $ | `'l1'` |\n",
    "    | L2   | $r(\\bm\\beta) = \\frac{1}{2} \\|\\bm\\beta\\|_2^2 = \\frac{1}{2} \\sum_{k=1}^K \\beta_k^2$ | `'l2'` |\n",
    "    | L1 and L2 | $r(\\bm\\beta) = \\rho \\|\\bm\\beta\\|_1 + \\frac{1-\\rho}{2} \\|\\bm\\beta\\|_2^2 $  | `'ElasticNet'` |\n",
    "    | None | | `None` |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create and estimate Logistic regression model\n",
    "# lr = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Visually assess model predictions\n",
    "\n",
    "-   Visually inspect decision boundary (possible for 2D case, not possible in general)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lecture12_classifiers import plot_decision_boundary\n",
    "\n",
    "# Create x-values used to evaluate decisions\n",
    "xvalues = np.linspace(0, 1, 1000)\n",
    "\n",
    "ax = plot_classes(X_train, y_train, X_test, y_test)\n",
    "plot_decision_boundary(ax, xvalues, lr)\n",
    "ax.set_title('Classification with logistic regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Assess model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lecture12_classifiers import plot_generic_confusion_matrix\n",
    "plot_generic_confusion_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   **Accuracy:** implemented in [`accuracy_score()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)\n",
    "    $$\n",
    "    ACC = \\frac{TP + TN}{FP + FN + TP + TN}\n",
    "    $$\n",
    "-   **Precision:** implemented in [`precision_score()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html)\n",
    "    $$\n",
    "    PRE = \\frac{TP}{TP + FP}\n",
    "    $$\n",
    "-   **Recall:** implemented in [`recall_score()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html)\n",
    "    $$\n",
    "    REC = \\frac{TP}{FN + TP}\n",
    "    $$\n",
    "-   **F1 score:** implemented in [`f1_score()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)\n",
    "    $$\n",
    "    F1 = 2 \\frac{PRE \\cdot REC}{PRE + REC}\n",
    "    $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Predict y on test sample\n",
    "# y_test_pred = \n",
    "\n",
    "# TODO: Compute accuracy\n",
    "# TODO: Compute precision\n",
    "# TODO: Compute recall\n",
    "# TODO: Compute F1 score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6: Plot the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "# Plot confusion matrix from predicted values\n",
    "ConfusionMatrixDisplay.from_predictions(\n",
    "    y_test,\n",
    "    y_test_pred,\n",
    "    colorbar=False,\n",
    "    cmap='Blues',\n",
    "    text_kw={'fontsize': 12, 'fontweight': 'bold'},\n",
    ").ax_.set_title('Confusion matrix for linear index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Fitting a model with polynomials\n",
    "\n",
    "-   Estimate model with polynomial interactions:\n",
    "    $$\n",
    "    z_i = \\mu + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\beta_3 x_{1i} x_{2i} + \\beta_4 x_{1i}^2 + \\beta_5 x_{2i}^2 + \\dots\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Estimate logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Maximum polynomial degree\n",
    "degree = 5\n",
    "\n",
    "# Create pipeline with polynomial features and logistic regression\n",
    "pipe_lr = make_pipeline(\n",
    "    PolynomialFeatures(degree=degree, include_bias=False),\n",
    "    StandardScaler(),\n",
    "    # TODO: Add Logistic regression estimator\n",
    ")\n",
    "\n",
    "# TODO: Fit logistic regression with polynomial features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Visually inspect decision boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create x-values used to evaluate decisions\n",
    "xvalues = np.linspace(0, 1, 1000)\n",
    "ax = plot_classes(X_train, y_train, X_test, y_test)\n",
    "plot_decision_boundary(ax, xvalues, pipe_lr)\n",
    "ax.set_title('Classification with logistic regression (polynomials)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Compute accuracy metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict y on test sample\n",
    "y_test_pred = pipe_lr.predict(X_test)\n",
    "\n",
    "acc_test = accuracy_score(y_test, y_test_pred)\n",
    "pre_test = precision_score(y_test, y_test_pred)\n",
    "rec_test = recall_score(y_test, y_test_pred)\n",
    "\n",
    "print(f'Accuracy on test sample: {acc_test:.3f}')\n",
    "print(f'Precision on test sample: {pre_test:.3f}')\n",
    "print(f'Recall on test sample: {rec_test:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Plot confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix from predicted values\n",
    "ConfusionMatrixDisplay.from_predictions(\n",
    "    y_test,\n",
    "    y_test_pred,\n",
    "    colorbar=False,\n",
    "    cmap='Blues',\n",
    "    text_kw={'fontsize': 12, 'fontweight': 'bold'},\n",
    ").ax_.set_title('Confusion matrix for linear index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Cross-validating the penalty term\n",
    "\n",
    "- Regularization strength $C$ can be cross-validated with\n",
    "[LogisticRegressionCV](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html)\n",
    "- `LogisticRegressionCV` does not support pipelines   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Perform cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "# Pipeline to create polynomial features and standardize them\n",
    "transform = make_pipeline(\n",
    "    PolynomialFeatures(degree=degree, include_bias=False), StandardScaler()\n",
    ")\n",
    "\n",
    "# Fit and transform training data\n",
    "# X_train_poly = transform.fit_transform(X_train)\n",
    "\n",
    "\n",
    "# TODO: Create and run Logistic regression cross-validation\n",
    "# lrcv =\n",
    "\n",
    "# TODO: Run cross-validation\n",
    "\n",
    "# TODO: Store and report best C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Re-run model with optimal C (optional)\n",
    "\n",
    "-   Not strictly need, could directly use fitted `LogisticRegressionCV` object\n",
    "-   But `LogisticRegressionCV` does not support pipelines..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_opt = make_pipeline(\n",
    "    PolynomialFeatures(degree=degree, include_bias=False),\n",
    "    StandardScaler()\n",
    "    # TODO: Add Logistic regression estimator with optimal C\n",
    ")\n",
    "\n",
    "# TODO: Fit model\n",
    "# lr_opt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Visually inspect decision boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create x-values used to evaluate decisions\n",
    "xvalues = np.linspace(0, 1, 1000)\n",
    "ax = plot_classes(X_train, y_train, X_test, y_test)\n",
    "plot_decision_boundary(ax, xvalues, lr_opt)\n",
    "ax.set_title('Classification with logistic regression (CV)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Compute accuracy metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict y on test sample\n",
    "y_test_pred = lr_opt.predict(X_test)\n",
    "\n",
    "# Compute accuracy of cross-validated model on test data\n",
    "acc_test = accuracy_score(y_test, y_test_pred)\n",
    "pre_test = precision_score(y_test, y_test_pred)\n",
    "rec_test = recall_score(y_test, y_test_pred)\n",
    "\n",
    "print(f'Accuracy on test sample: {acc_test:.3f}')\n",
    "print(f'Precision on test sample: {pre_test:.3f}')\n",
    "print(f'Recall on test sample: {rec_test:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Random forest\n",
    "\n",
    "-   Averages results from many decision trees\n",
    "-   Leads to less overfitting\n",
    "-   Fully nonlinear classifier, usually does not require polynomial interactions, dummy variable encoding, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Create estimation sample\n",
    "\n",
    "-   Recreate estimation sample from earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lecture12_classifiers import create_class_data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Sample size\n",
    "N = 100\n",
    "\n",
    "# Standard deviation of noise\n",
    "sigma_eps = 0.2\n",
    "\n",
    "# Create demo data set for classification\n",
    "X, y = create_class_data(N=N, sigma=sigma_eps)\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, test_size=0.3, random_state=1234\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Estimate Random forest\n",
    "\n",
    "-   Implemented in [RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n",
    "-   Important hyperparameters:\n",
    "\n",
    "    -   `n_estimators`: number of trees to grow\n",
    "    -   `max_depth`: maximum depth of individual trees\n",
    "\n",
    "-   Other important arguments:\n",
    "    -   `random_state`: trees are grown on bootstrapped samples (involves RNG)\n",
    "    -   `n_jobs`: number of parallel processes to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fit Random forest classifier\n",
    "# forest = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Visually inspect decision boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plot_classes(X_train, y_train, X_test, y_test)\n",
    "plot_decision_boundary(ax, xvalues, forest)\n",
    "ax.set_title('Classification with random forest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Compute accuracy metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Predict y on training and test samples\n",
    "y_train_pred_forest = forest.predict(X_train)\n",
    "y_test_pred_forest = forest.predict(X_test)\n",
    "\n",
    "# Compute accuracy on training and test samples\n",
    "acc_train = accuracy_score(y_train, y_train_pred_forest)\n",
    "acc_test = accuracy_score(y_test, y_test_pred_forest)\n",
    "\n",
    "print(f'Accuracy on training sample: {acc_train:.3f}')\n",
    "print(f'Accuracy on test sample: {acc_test:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validating Random forest hyperparameters\n",
    "\n",
    "-   No dedicated cross-validation class for Random forest available\n",
    "-   Use generic [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Run grid search\n",
    "\n",
    "-   Candidate grid for each parameter is specified using `param_grid` argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# TODO: Define grid for max_depth\n",
    "\n",
    "# TODO: Define grid for n_estimators\n",
    "\n",
    "# TODO: Create and run GridSearchCV\n",
    "# forest_cv = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report best hyperparameters\n",
    "print(f'Best accuracy: {forest_cv.best_score_:.3f}')\n",
    "print(f'Best parameters: {forest_cv.best_params_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Visually inspect decision boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plot_classes(X_train, y_train, X_test, y_test)\n",
    "plot_decision_boundary(ax, xvalues, forest_cv)\n",
    "ax.set_title(f'Classification with random forest (max depth: {forest_cv.best_params_[\"max_depth\"]})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Compute accuracy metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_forest = forest.predict(X_train)\n",
    "y_test_pred_forest = forest.predict(X_test)\n",
    "\n",
    "acc_train = accuracy_score(y_train, y_train_pred_forest)\n",
    "acc_test = accuracy_score(y_test, y_test_pred_forest)\n",
    "\n",
    "print(f'Accuracy on training sample: {acc_train:.3f}')\n",
    "print(f'Accuracy on test sample: {acc_test:.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FIE463",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
