{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to scikit-learn\n",
    "\n",
    "## Supervised learning\n",
    "\n",
    "Goal: learn the relationship $y = f(X)$ between:\n",
    "\n",
    "1.  input data (independent variables, features, covariates, predictors, or $X$ variables)\n",
    "2.  output data (dependent variable, outcome, target, or $y$ variable)\n",
    " \n",
    "Two broad types of approaches covered in lectures 11 & 12:\n",
    "\n",
    "1. **Regression**: output data takes on continuous values\n",
    "2. **Classification**: output data is restricted to a few values, often called categories or labels\n",
    "\n",
    "### Machine learning vs. econometrics/statistics\n",
    "\n",
    "- \"Classical\" econometrics:\n",
    "  - Focus on inference, establishing causal relationships\n",
    "  - Emphasis on unbiased and consistent estimators\n",
    "\n",
    "- Machine learning:\n",
    "  - Focus on out-of-sample predictive performance\n",
    "  - Bias/variance trade-off: accept biased estimators that have lower variance \n",
    "  - Usually includes some form of regularization / penalty / shrinkage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Outline\n",
    "\n",
    "### This week\n",
    "\n",
    "-   Linear regression without regularization\n",
    "-   Training/test sample splits\n",
    "-   Pipelines (data pre-processing & estimation)\n",
    "-   Hyperparameter tuning via cross-validation\n",
    "-   Principal component regression (if we get there)\n",
    "\n",
    "### Next week\n",
    "\n",
    "-   Regression with regularization: Ridge & Lasso\n",
    "-   Classification: Logistic regression, Support Vector Machines, Random forest"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Univariate linear regression\n",
    "\n",
    "Simple linear model with single independent variable $x$:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{Econometrics:} \\qquad y_i &= \\alpha + \\beta x_i + \\epsilon_i \\\\\n",
    "\\text{Machine learning:} \\qquad y_i &= b + w x_i + \\epsilon_i\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "#### Terminology\n",
    "\n",
    "- $y$: dependent variable, response variable, outcome, target\n",
    "- $x$: independent variables, features, covariates, predictors\n",
    "- $\\epsilon$: error term\n",
    "- $\\alpha$, $b$: intercept or bias (ML)\n",
    "- $\\beta$, $w$: slope coefficient or weight (ML)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linearity assumption\n",
    "\n",
    "- Model is assumed to be linear **in coefficients**, not in $x$\n",
    "- Linear models include the following:\n",
    "    $$\n",
    "    \\begin{aligned}\n",
    "     y_i &= \\alpha + \\beta_1 x_i + \\beta_2 x_i^2 + \\epsilon_i \\\\\n",
    "     \\log y_i &= \\alpha + \\beta \\log x_i + \\epsilon_i\n",
    "    \\end{aligned}\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Estimation of linear models (OLS)\n",
    "\n",
    "- Goal: Minimize loss function given by **sum of squared residuals**:\n",
    "    $$\n",
    "    L(\\alpha, \\beta) = \n",
    "        \\sum_i^N \\Bigl(y_i - \\alpha - \\beta x_i \\Bigr)^2\n",
    "    $$\n",
    "- **Estimates:** Parameters that minimize $L$, usually denoted $\\widehat{\\alpha}$, $\\widehat{\\beta}$\n",
    "- **Predicted values** for given $x_i$:\n",
    "    $$\n",
    "    \\widehat{y}_i = \\widehat{\\alpha} + \\widehat{\\beta} x_i\n",
    "    $$\n",
    "- **Prediction error** for given $y_i$, $x_i$:\n",
    "    $$\n",
    "    \\widehat{\\epsilon}_i = y_i - \\widehat{y_i} = y_i - \\widehat{\\alpha} - \\widehat{\\beta} x_i\n",
    "    $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Univariate linear regressions with scikit-learn\n",
    "\n",
    "#### Step 1: Generate demo sample \n",
    "\n",
    "Assume true model is given as follows:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "y_i &= \\alpha + \\beta x_i + \\epsilon_i \\\\\n",
    "\\epsilon_i &\\stackrel{\\text{iid}}{\\sim} N(0, \\sigma^2) \\\\\n",
    "\\alpha &= 1,\\quad \\beta = \\frac{1}{2},\\quad \\sigma = 0.7 \\\\ \n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "\n",
    "def create_sample(N, alpha, beta, sigma, rng=None):\n",
    "    \"\"\"\n",
    "    Create sample for univariate linear regression\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate RNG instance with seed 1234\n",
    "    rng = default_rng(seed=1234)\n",
    "\n",
    "    # Use x that are uniformly distributed on [0, 10]\n",
    "    x = rng.uniform(0, 10, size=N)\n",
    "\n",
    "    # Normally distributed errors\n",
    "    epsilon = rng.normal(scale=sigma, size=N)\n",
    "\n",
    "    # Create outcome variable\n",
    "    y = alpha + beta * x + epsilon\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample size\n",
    "N = 50\n",
    "\n",
    "# Model parameters\n",
    "alpha = 1.0\n",
    "beta = 0.5\n",
    "sigma = 0.7\n",
    "\n",
    "# Create sample\n",
    "x, y = create_sample(N=50, alpha=1.0, beta=0.5, sigma=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Estimate model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use [`LinearRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)\n",
    "- Important: `scikit-learn` *always* expects a matrix of explanatory variables (even if there is only one)\n",
    "- Estimated coefficients are stored in `intercept_` and `coef_` attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create LinearRegression instance\n",
    "# 2. Fit the model\n",
    "# 3. Recover and report estimated alpha_hat and beta_hat coefficients\n",
    "# 4. Predict y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Plot sample vs. predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(5, 3.5))\n",
    "\n",
    "# Plot true relationship\n",
    "plt.axline(\n",
    "    (0.0, alpha), slope=beta, lw=1.0, ls='--', c='black', \n",
    "    label=r'$y = \\alpha + \\beta x$'\n",
    ")\n",
    "\n",
    "# Plot regression line\n",
    "plt.axline(\n",
    "    (0.0, alpha_hat), slope=beta_hat, lw=1.0, c='darkorange', \n",
    "    label=r'$\\widehat{y} = \\widehat{\\alpha} + \\widehat{\\beta} x$'\n",
    ")\n",
    "\n",
    "# Plot raw data\n",
    "plt.scatter(\n",
    "    x, y, s=20, color='none', edgecolor='steelblue', alpha=1.0, lw=0.75, label='y'\n",
    ")\n",
    "\n",
    "# Plot lines connecting true and predicted values for\n",
    "# each observation\n",
    "for i in range(len(x)):\n",
    "    plt.plot([x[i], x[i]], [y[i], yhat[i]], lw=0.5, ls=':', c='black', alpha=0.9)\n",
    "\n",
    "# Add annotations\n",
    "plt.xlabel('Explanatory variable: x')\n",
    "plt.ylabel(r'Response variable: y, $\\widehat{y}$')\n",
    "plt.axhline(alpha_hat, lw=0.5, c='black')\n",
    "plt.legend(loc='best')\n",
    "plt.yticks((alpha_hat,), (r'$\\widehat{\\alpha}$',))\n",
    "\n",
    "plt.title('Univariate linear regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<h3> Your turn</h3>\n",
    "<ol>\n",
    "    <li>Recreate and re-estimate the model for samples of size 10, 50, 100, 1000, and 10000.</li>\n",
    "    <li>Print the estimated values of α and β. Can you say something about the deviation from the true values as the sample size increases?</li>\n",
    "</ol>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Training and test samples\n",
    "\n",
    "- ML mostly deals with prediction\n",
    "- Estimate model on **training sample**\n",
    "- Evaluate prediction on **test sample**\n",
    "- Avoids **overfitting** on training sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Ames housing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: Load and pre-process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use local data/ directory\n",
    "DATA_PATH = '../../data'\n",
    "\n",
    "# Load data directly from GitHub\n",
    "# DATA_PATH = 'https://raw.githubusercontent.com/richardfoltyn/FIE463-V25/main/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file = f'{DATA_PATH}/ames_houses.csv'\n",
    "df = pd.read_csv(file)\n",
    "\n",
    "# List columns present in DataFrame\n",
    "df.info(show_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Target variable `SalePrice` (in thousand USD)\n",
    "- Explanatory variable `LivingArea` (in m²)\n",
    "- Drop outliers with living area above 350m²"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Train-test split\n",
    "\n",
    "- Use [`train_test_split()`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n",
    "    - Specify `test_size` or `train_size`\n",
    "    - Set `random_state` for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Estimate model on training sample\n",
    "\n",
    "- Linear model:\n",
    "    $$\n",
    "    SalePrice_i = \\alpha + \\beta \\cdot LivingArea_i + \\epsilon_i\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Plot sample & fitted relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(5, 3.5))\n",
    "\n",
    "# Plot regression line\n",
    "plt.axline((0.0, intercept), slope=slope, lw=1.5, c='black', \n",
    "    label=r'$\\widehat{y} = \\widehat{\\alpha} + \\widehat{\\beta} x$')\n",
    "\n",
    "# Plot training data\n",
    "plt.scatter(X_train, y_train, s=15, color='none', edgecolor='steelblue', \n",
    "    alpha=0.8, lw=0.5, label=r'$y_{train}$')\n",
    "\n",
    "# Plot test data\n",
    "plt.scatter(X_test, y_test, s=20, color='none', edgecolor='darkorange', \n",
    "    alpha=1.0, lw=0.5, label=r'$y_{test}$')\n",
    "\n",
    "# Add annotations\n",
    "plt.xlabel('Living area in $m^2$')\n",
    "plt.ylabel(r'Sales price in thousand $')\n",
    "plt.title('Fitted univariate linear regression')\n",
    "plt.legend(loc='best')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Compute and plot prediction errors on test sample\n",
    "\n",
    "- Prediction errors:\n",
    "$$\n",
    "\\epsilon_i = y_i - \\widehat{y}_i = y_i - \\widehat{\\alpha} - \\widehat{\\beta} x_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 3.5))\n",
    "\n",
    "# Scatter plot of prediction errors\n",
    "plt.scatter(X_test, error, s=20, color='none', edgecolor='red', alpha=1.0, lw=0.75)\n",
    "\n",
    "# Add annotations\n",
    "plt.xlabel('Living area in $m^2$')\n",
    "plt.ylabel(r'Thousand $')\n",
    "plt.title('Prediction errors in test sample')\n",
    "plt.axhline(0.0, lw=0.75, ls='--', c='black')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Multivariate linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Data with several explanatory variables\n",
    "\n",
    "$$\n",
    "y_i = \\mathbf{x}_i'\\mathbf{\\beta} + \\epsilon_i\n",
    "$$\n",
    "\n",
    "- Regressors are now given as **vector** $\\mathbf{x}$\n",
    "- Coefficient **vector** $\\bm\\beta$ to be estimated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Example: Ames housing data\n",
    "\n",
    "- Target variable `SalePrice` (in thousand USD)\n",
    "- Explanatory variables `LivingArea` (in m²), `LotArea` (in m²)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: Load and pre-process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_ames_data(features, target):\n",
    "    \"\"\"\n",
    "    Load Ames housing data from CSV file and select features and target variable\n",
    "    \"\"\"\n",
    "\n",
    "    # Load data from CSV file\n",
    "    file = f'{DATA_PATH}/ames_houses.csv'\n",
    "    df = pd.read_csv(file)\n",
    "\n",
    "    # Drop any missing obs in selected columns\n",
    "    columns = features + [target]\n",
    "    df = df.dropna(subset=columns)\n",
    "\n",
    "    # Select features and target variable\n",
    "    X = df[features]\n",
    "    y = df[target]\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check for outliers using scatter plot\n",
    "- Drop observations with living area above 350m² or lot area above 5,000m²"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2: Train-test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3: Fit the model\n",
    "\n",
    "- Multivariate linear model:\n",
    "    $$\n",
    "    SalePrice_i = \\alpha + \\beta_0 LivingArea_i + \\beta_1 LotArea_i + \\epsilon_i\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 4: Plot prediction errors on test sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(y_test, errors, s=20, lw=0.75, color='none', edgecolor='red')\n",
    "plt.axhline(0.0, lw=0.75, ls='--', c='black')\n",
    "plt.title('Prediction errors in test sample')\n",
    "plt.xlabel(r'$y_{test}$')\n",
    "plt.ylabel('Thousand $')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Polynomial features\n",
    "\n",
    "- Example: Linear model with cubic polynomial in $x$ as explanatory variable:\n",
    "    $$\n",
    "    y_i = \\alpha + \\beta_0 x_i + \\beta_1 x_i^2 + \\beta_2 x_i^3 + \\epsilon_i\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Ames housing data with interactions\n",
    "\n",
    "- Polynomials with interactions of $x$ and $z$:\n",
    "    $$\n",
    "    p(x,z) = \\beta_0 + \\beta_1 x + \\beta_2 z + \\beta_3 x^2 + \\beta_4 x \\cdot z + \\beta_5 z^2\n",
    "    $$\n",
    "    - $x$: `LivingArea`\n",
    "    - $z$: `LotArea`\n",
    "- Model given by\n",
    "    $$\n",
    "    SalePrice_i = p(LivingArea_i, LotArea_i) + \\epsilon_i\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: Create polynomial features\n",
    "\n",
    "- Create with [`PolynomialFeatures`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html)\n",
    "- Pass `include_bias=True` to include intercept (constant)\n",
    "- Exponents stored in `powers_` attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2: Fit model\n",
    "- If polynomial has constant, fit linear model with `fit_intercept=False`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3: Plot prediction errors on test sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_test, errors, s=20, lw=0.75, color='none', edgecolor='red')\n",
    "plt.axhline(0.0, lw=0.75, ls='--', c='black')\n",
    "plt.xlabel(r'$y_{test}$')\n",
    "plt.ylabel(r'Thousand $')\n",
    "plt.title('Prediction errors in test sample')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Using scikit-learn pipelines\n",
    "\n",
    "- Manually preprocessing variables is error prone \n",
    "- Can be automated using **pipelines**\n",
    "- Two equivalent alternatives to create pipelines:\n",
    "    1. Create an instance of the \n",
    "        [`Pipeline`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline) class\n",
    "    2. Use the \n",
    "        [`make_pipeline()`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html) convenience function\n",
    "\n",
    "    Pipeline names can be accessed using `named_steps` attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Load and pre-process Ames housing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Ames housing data\n",
    "features = ['LivingArea', 'LotArea']\n",
    "target = 'SalePrice'\n",
    "\n",
    "X, y = load_ames_data(features, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Create pipeline\n",
    "\n",
    "- Combine polynomial feature creation and estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Fit model using pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Plot prediction errors on test sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_test, errors, s=20, lw=0.75, color='none', edgecolor='red')\n",
    "plt.axhline(0.0, lw=0.75, ls='--', c='black')\n",
    "plt.xlabel(r'$y_{test}$')\n",
    "plt.ylabel(r'Thousand $')\n",
    "plt.title('Prediction errors in test sample')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Evaluating the model fit\n",
    "\n",
    "- Need metric (score) to evaluate model fit\n",
    "- **Mean squared error (MSE):**\n",
    "    $$\n",
    "    MSE = \\frac{1}{N} \\sum_{i=1}^N \\bigl(y_i - \\widehat{y}_i\\bigr)^2\n",
    "    $$\n",
    "- **Root mean squared error (RMSE)**:\n",
    "    $$\n",
    "    RMSE = \\sqrt{MSE} = \\sqrt{\\frac{1}{N} \\sum_{i=1}^N \\bigl(y_i - \\widehat{y}_i\\bigr)^2}\n",
    "    $$\n",
    "- **Coefficients of determination ($R^2$)**: bounded within $[0,1]$ on training sample\n",
    "    $$\n",
    "    R^2 = 1 - \\frac{MSE}{\\widehat{Var}(y)}\n",
    "    $$\n",
    "\n",
    "Convenience functions in `scikit-learn`:\n",
    "\n",
    "-   [`mean_squared_error()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html#sklearn.metrics.mean_squared_error)\n",
    "-   [`root_mean_squared_error()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.root_mean_squared_error.html) for the RMSE; and\n",
    "-   [`r2_score()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html#sklearn.metrics.r2_score) for the $R^2$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Optimizing hyperparameters with cross-validation\n",
    "\n",
    "### Outline of hyperparameter tuning\n",
    "\n",
    "- Hyperparameters: additional parameters that are **not** estimated, e.g., polynomial degree used earlier\n",
    "- How do we find optimal values for such parameters?\n",
    "    1. Estimate model for different values of hyperparameters\n",
    "    2. Pick best-performing model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CV split](cv_split.svg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Example: Tuning of the polynomial degree\n",
    "\n",
    "- Same setting as earlier:\n",
    "    $$\n",
    "    p(x,z) = \\beta_0 + \\beta_1 x + \\beta_2 z + \\beta_3 x^2 + \\beta_4 x \\cdot z + \\beta_5 z^2\n",
    "    $$\n",
    "    - $x$: `LivingArea`\n",
    "    - $z$: `LotArea`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Load and pre-process Ames housing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Ames housing data\n",
    "features = ['LivingArea', 'LotArea']\n",
    "target = 'SalePrice'\n",
    "\n",
    "X, y = load_ames_data(features, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Perform cross-validation\n",
    "- Find best polynomial degree $d = 0,\\dots,5$ using K-fold CV with 10 folds\n",
    "- Use [`KFold`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold) to split sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "degrees = np.arange(6)\n",
    "\n",
    "for d in degrees:\n",
    "\n",
    "    # 1. Create pipeline with polynomial features and linear regression\n",
    "\n",
    "\n",
    "    # Create 10 folds\n",
    "    folds = KFold(n_splits=10)\n",
    "\n",
    "    # Iterate through all combinations of training/test data\n",
    "    for itrain, itest in folds.split(X, y):\n",
    "\n",
    "        # Extract training data for this split\n",
    "        X_train = X.iloc[itrain]\n",
    "        y_train = y.iloc[itrain]\n",
    "\n",
    "        # Extract test (or validation) data for this split\n",
    "        X_test = X.iloc[itest]\n",
    "        y_test = y.iloc[itest]\n",
    "\n",
    "        # 2. Fit model on training data for given degree\n",
    "\n",
    "        # 3. Predict response on train and test data\n",
    "\n",
    "        # 4. Compute RMSE as model fit measure\n",
    "\n",
    "        # 5. Store RMSE for current split\n",
    "\n",
    "    # 6. Store average MSE for current polynomial degree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Find hyperparameter with lowest RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Plot validation curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(degrees, rmse_train_mean, c='black', marker='x', ms=4, alpha=0.7, label='Training')\n",
    "plt.plot(degrees, rmse_test_mean, c='steelblue', marker='o', ms=4, label='Validation')\n",
    "plt.xlabel('Polynomial degree')\n",
    "plt.ylabel('Cross-validated RMSE')\n",
    "plt.xticks(degrees)\n",
    "plt.axvline(imin, ls=':', lw=0.75, c='black')\n",
    "plt.title('Validation curve')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Automating cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Automatically computing scores on all folds\n",
    "\n",
    "- Helper function [`cross_val_score()`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html):\n",
    "    - Select number of folds: `cv=10`\n",
    "    - Select metric: `scoring='neg_root_mean_squared_error'`\n",
    "    - List of available metrics (scores): `sklearn.metrics.get_scorer_names()`\n",
    "        or [documentation](https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "\n",
    "degrees = np.arange(5)\n",
    "\n",
    "rmse_mean = []\n",
    "rmse_std = []\n",
    "\n",
    "for d in degrees:\n",
    "\n",
    "    # Create pipeline to transform and fit the model. Pipeline depends\n",
    "    # on polynomial degree!\n",
    "    if d == 0:\n",
    "        # Intercept-only model\n",
    "        pipe = make_pipeline(\n",
    "            PolynomialFeatures(degree=d, include_bias=True),\n",
    "            LinearRegression(fit_intercept=False),\n",
    "        )\n",
    "    else:\n",
    "        # Model with polynomial terms: need to standardize features for \n",
    "        # numerical stability\n",
    "        pipe = make_pipeline(\n",
    "            PolynomialFeatures(degree=d, include_bias=False),\n",
    "            StandardScaler(),\n",
    "            LinearRegression(fit_intercept=True),\n",
    "        )\n",
    "\n",
    "    # ADD CALL TO cross_val_score HERE\n",
    "\n",
    "    # Function returns NEGATIVE RMSE, correct this here!\n",
    "    rmse_mean.append(np.mean(-score))\n",
    "    \n",
    "# Convert to NumPy array\n",
    "rmse_mean = np.array(rmse_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Automatically computing the validation curve\n",
    "\n",
    "- Even more automated: [`validation_curve()`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.validation_curve.html)\n",
    "- Specify parameter to tune, parameter range, and scoring metric\n",
    "- `validation_curve()` returns scores for training and tests sets for all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "# Candidate polynomial degrees (ignore intercept-only model)\n",
    "degrees = np.arange(1, 6)\n",
    "\n",
    "# 1. Create estimator pipeline with hyperparameter to tune\n",
    "\n",
    "# 2. Call validation_curve()\n",
    "\n",
    "# 3. Average scoring metric across all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute mean for each degree (scores returned by function are \n",
    "# NEGATIVE RMSEs)\n",
    "rmse_train_mean = np.mean(-train_scores, axis=1)\n",
    "rmse_test_mean = np.mean(-test_scores, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(degrees, rmse_train_mean, c='black', marker='x', ms=4, alpha=0.7, label='Training')\n",
    "plt.plot(degrees, rmse_test_mean, c='steelblue', marker='o', ms=4, label='Validation')\n",
    "plt.xlabel('Polynomial degree')\n",
    "plt.ylabel('Cross-validated RMSE')\n",
    "plt.xticks(degrees)\n",
    "plt.axvline(imin, ls=':', lw=0.75, c='black')\n",
    "plt.title('Validation curve')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Advanced pipelines and cross-validation: Principal component regression (PCR)\n",
    "\n",
    "- Use [Principal component analysis](https://en.wikipedia.org/wiki/Principal_component_analysis)\n",
    "    to address overfitting and multicollinearity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Load and pre-process Ames housing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Ames data with additional features\n",
    "features = ['LivingArea', 'LotArea', 'YearBuilt', 'OverallQuality']\n",
    "target = 'SalePrice'\n",
    "\n",
    "X, y = load_ames_data(features, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Create estimator pipeline & fit model\n",
    "\n",
    "- Implemented in [`PCA`](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)\n",
    "- PCA is sensitive to scaling, need to apply `StandardScaler` first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# ADD estimator pipeline with 4 steps: polynomial features, standardization,\n",
    "# PCA, linear regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Report number of principal components used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract fraction of variance explained by each principal component.\n",
    "# Note that the name of the pipeline step is the one assigned when creating the \n",
    "# pipeline.\n",
    "var_expl = pipe_pcr.named_steps['pca'].explained_variance_ratio_\n",
    "var_expl\n",
    "\n",
    "# Use pandas plotting functions to create bar plot\n",
    "pd.Series(var_expl, index=[f'PC{i+1}' for i in range(len(var_expl))]).plot.bar(\n",
    "    ylabel='Fraction of variance', title='PCA explained variance', rot=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Cross-validation for PCR\n",
    "\n",
    "- Two hyperparameters: \n",
    "\n",
    "    1.  Maximum polynomial degree for feature creation\n",
    "    2.  Number of principal components to use (or explained fraction of variance)\n",
    "\n",
    "-  Use [`GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) to run cross-validation on hyperparameter grid\n",
    "-   Parameters and values are specified using `param_grid` argument (dictionary)\n",
    "-   Results stored in `best_params_` and `best_score_` attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "# Define grid of hyperparameters\n",
    "param_grid = {\n",
    "    'poly__degree': np.arange(1, 6),\n",
    "    'pca__n_components': np.linspace(0.9, 0.999999, 20),\n",
    "}\n",
    "\n",
    "# Create GridSearchCV object\n",
    "pcr_cv = GridSearchCV(\n",
    "    estimator=pipe_pcr,                     # Estimator pipeline\n",
    "    param_grid=param_grid,                  # Grid of hyperparameters\n",
    "    scoring='neg_root_mean_squared_error',  # Scoring criterion\n",
    "    cv=10,                                  # Number of folds for CV\n",
    "    n_jobs=-1,                              # Use all available CPU cores\n",
    "    refit=True                              # Refit model with best hyperparameters\n",
    ")\n",
    "\n",
    "# Run grid search\n",
    "pcr_cv.fit(X, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
