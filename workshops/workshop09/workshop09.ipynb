{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Daily returns of the magnificent seven\n",
    "\n",
    "In this exercise, you are asked to analyze the weekly stockmarket returns\n",
    "of the so-called magnificent 7 which are some of the most successful tech companies \n",
    "of the last decades years:\n",
    "Apple (AAPL), Amazon (AMZN), Google (GOOG), Meta (META), Microsoft (MSFT), Nvidia (NVDA), and Tesla (TSLA).\n",
    "\n",
    "1. Load the CSV data from \n",
    "    `../../data/stockmarket/magnificent7.csv`. \n",
    "    Inspect to first few rows\n",
    "    to familiarize yourself with the columns present in the `DataFrame`.\n",
    "\n",
    "    Keep only the columns `Date`, `Ticker`, `Open`, and `Close`.\n",
    "\n",
    "2. You want to compute weekly returns for each of the 7 stocks. To this end, \n",
    "    you need to reshape the `DataFrame` so that `Date` is the index and the\n",
    "    remaining dimensions are in (hierarchical) columns.\n",
    "\n",
    "    One way to achieve this is to use the \n",
    "    [`pivot()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.pivot.html)\n",
    "    functions. Call this function with the arguments `index='Date'` and \n",
    "    `columns='Ticker'` and inspect the result.\n",
    "\n",
    "    This should generate a hierarchical column index with `Open` and `Close`\n",
    "    and the top level.\n",
    "\n",
    "    Drop all rows with any missing values which arise because these\n",
    "    stocks have been listed at different points in time.\n",
    "\n",
    "3.  Your data is now in a format that can be resampled to weekly frequency.\n",
    "    Use \n",
    "    [`resample()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.resample.html)\n",
    "    to convert the data to weekly observations.\n",
    "\n",
    "    Compute the weekly returns as the relative difference between the \n",
    "    *first* `Open` quote and the *last* `Close` quote for each ticker in each week.\n",
    "\n",
    "    *Hint:* You should use `resample('W-MON')` so that the resampled weeks\n",
    "    begin on Mondays (as opposed to the default Sundays).\n",
    "\n",
    "    *Hint:* For example, to select the first `Open` value in each week, you should\n",
    "    use `resample('W-MON')['Open'].first()`.\n",
    "\n",
    "4.  Create a 3-by-3 figure and plot the weekly returns you computed for each ticker as a histogram,\n",
    "    using 25 bins (i.e., `bins=25` should be passed to the `hist()` function).\n",
    "\n",
    "    Since you have only 7 tickers but 9 subplots in the figure, the last \n",
    "    two remaining subplots should remain empty.\n",
    "\n",
    "    *Hint:* You can either use [`DataFrame.hist()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.hist.html) to plot the histogram, or Matplotlib's [`hist()`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.hist.html) function. In either case, you should add `density=True` such that the histogram is appropriately rescaled and comparable to the normal density.\n",
    "    \n",
    "5.  **[Advanced]**\n",
    "    Compare the histograms you created to the normal (Gaussian) probability \n",
    "    density function (PDF) to\n",
    "    get an idea how much weekly returns differ from a normal distribution.\n",
    "\n",
    "    First, compute mean and standard deviation for each ticker\n",
    "    and tabulate these.\n",
    "\n",
    "    Then add a line showing the normal PDF to each of the return histograms you created previously,\n",
    "    using the mean and standard deviation for each ticker.\n",
    "\n",
    "    *Hint:* Use the `pdf()` method of the [`scipy.stats.norm`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html#scipy.stats.norm) class to compute the normal density.\n",
    "\n",
    "6.  Finally, you are interested in how the weekly returns are correlated across \n",
    "    the 7 stocks. \n",
    "\n",
    "    Create a figure with 7-by-7 subplots showing the pairwise correlations \n",
    "    for each combination of stocks.\n",
    "\n",
    "    You can do this either with the\n",
    "    [`scatter_matrix()`](https://pandas.pydata.org/docs/reference/api/pandas.plotting.scatter_matrix.html) function contained in `pandas.plotting`, \n",
    "    or build the figure using Matplotlib functions.\n",
    "\n",
    "    **[Advanced]**\n",
    "    Additionally, use the \n",
    "    [DataFrame.corr()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.corr.html)\n",
    "    method to compute the pairwise correlation matrix. Extract these values\n",
    "    and add them as text to each of the 7-by-7 subplots\n",
    "    (e.g., the correlation between returns on AAPL and AMZN is about 0.43,\n",
    "    so this text should be added to the subplot showing the \n",
    "    scatter plot of AAPL vs. AMZN).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Exercise 2: Business cycle correlations\n",
    "\n",
    "Use the macroeconomic data from the folder `../../data/FRED` to solve the following tasks:\n",
    "\n",
    "1.  There are seven decade-specific files named `FRED_monthly_19X0.csv` where `X` identifies the decade (`X` takes on the values 5, 6, 7, 8, 9, 0, 1). Write a loop that reads in all seven files as DataFrames and store them in a list.\n",
    "\n",
    "    *Hint:* Recall that you can use \n",
    "    `pd.read_csv(..., index_col='DATE', parse_dates=['DATE'])` \n",
    "    to automatically parse strings stored in the `DATE` column as dates.\n",
    "\n",
    "2.  Use \n",
    "    [`pd.concat()`](https://pandas.pydata.org/docs/reference/api/pandas.concat.html) to concatenate these data sets into a single `DataFrame` and make sure \n",
    "    that `DATE` is set as the index.\n",
    "\n",
    "3.  You realize that your data does not include GDP since this variable is only reported at quarterly frequency.\n",
    "    Load the GDP data from the file `GDP.csv` and merge it with your monthly data using an _inner join_.\n",
    "4.  You want to compute how (percent) changes of the variables in your data correlate with percent changes in GDP.\n",
    "\n",
    "    1. Create a _new_ `DataFrame` which contains the percent changes in CPI and GDP (using \n",
    "    [`pct_change()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.pct_change.html)),\n",
    "    and the absolute changes for the remaining variables (using \n",
    "    [`diff()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.diff.html)).\n",
    "    \n",
    "    2.  Compute the correlation of the percent changes in GDP with the (percent) changes of all other variables (using [`corr()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.corr.html)). What does the sign and magnitude of the correlation coefficient tell you?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Exercise 3: Okun's law\n",
    "\n",
    "In this exercise, we investigate [Okun's law](https://en.wikipedia.org/wiki/Okun%27s_law) based on quarterly US data for each of the last seven decades.\n",
    "\n",
    "Okun's law relates unemployment to the output gap. One version (see Jones: Macroeconomics, 2019) is stated as follows:\n",
    "<a id=\"Okun\"></a> \n",
    "$$\n",
    "\\underbrace{u_t - \\overline{u}_{t}\\vphantom{\\left(\\frac{Y_t - \\overline{Y}_t}{\\overline{Y}_t}\\right)}}_{\\text{cyclical unempl.}} = \n",
    "\\alpha + \\beta \\;\n",
    "    \\underbrace{\\left(\\frac{Y_t - \\overline{Y}_t}{\\overline{Y}_t}\\right)}_{\\text{output gap}}\n",
    "\\tag{3.1}\n",
    "$$\n",
    "where $u_t$ is the unemployment rate, $\\overline{u}_{t}$ is the natural rate of \n",
    "unemployment, $Y_t$ is output (GDP) and $\\overline{Y}_{t}$ is potential output. \n",
    "We refer to $u_t-\\overline{u}_{t}$ as \"cyclical unemployment\" and to the term in \n",
    "parenthesis on the right-hand side as the \"output gap.\" Okun's law says that \n",
    "the coefficient $\\beta$ is negative, i.e., cyclical unemployment is \n",
    "higher when the output gap is low (negative) because the economy \n",
    "is in a recession.\n",
    "\n",
    "Use the FRED data in the `../../data/FRED` folder and \n",
    "perform the following tasks:\n",
    "\n",
    "1.  Load the time series stored in `GDP.csv` (real GDP), \n",
    "    `GDPPOT.csv` (real potential GDP),\n",
    "    `UNRATE.csv` (unemployment rate) and \n",
    "    `NROU.csv` (noncyclical rate of unemployment), where the last \n",
    "    series corresponds to the natural rate of unemployment mentioned above.\n",
    "\n",
    "    Combine these series into a single `DataFrame` so that each \n",
    "    represents a column, and keep only observations from from 1950-2019.\n",
    "    The resulting data should be at quarterly frequency since GDP is \n",
    "    only observed at these intervals.\n",
    "\n",
    "    *Hint:* Use `pd.read_csv(..., index_col='DATE', parse_dates=['DATE'])`\n",
    "    to automatically parse strings stored in the `DATE` column as dates\n",
    "    and set it as the index.\n",
    "\n",
    "2.  Compute the output gap and cyclical unemployment rate as defined above and \n",
    "    add them as columns to the `DataFrame`.\n",
    "\n",
    "    Plot these variables in a scatter plot\n",
    "    with the output gap on the $x$-axis and the \n",
    "    cyclical unemployment on the $y$-axis. Does Okun's law hold over the\n",
    "    sample period?\n",
    "\n",
    "3.  You wonder if the relationship has changed over the last decades. \n",
    "    To answer this question, create a new column `Decade` which stores the decade of each observation,\n",
    "    e.g., 1950, 1960, etc.\n",
    "    Verify that each decade has 40 quarterly observations in your data.\n",
    "\n",
    "    *Hint:* Since you have a date index, the calendar year can be \n",
    "    retrieved from the attribute `df.index.year`.\n",
    "\n",
    "4.  Create a figure with 3-by-3 subplots showing the same scatter plot as \n",
    "    above, but separately for each decade. Since we have data for only 7\n",
    "    decades, the last two subplots should remain empty.\n",
    "\n",
    "5.  **[Advanced]** \n",
    "    Write a function `regress_okun()` which accepts a `DataFrame` \n",
    "    containing a decade-spefic \n",
    "    sub-sample as the only argument, and estimates the coefficients \n",
    "    $\\alpha$ (the intercept) and $\\beta$ (the slope) of the above regression\n",
    "    equation [(3.1)](#Okun).\n",
    "\n",
    "    This function should return a `Series` with two elements\n",
    "    which store the intercept and slope.\n",
    "\n",
    "    To run the regression by decade, group the data by `Decade` and call the [`apply()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.GroupBy.apply.html) method, passing \n",
    "    `regress_okun` you wrote as the argument.\n",
    "\n",
    "\n",
    "    *Hint:* Use NumPy's [`lstsq()`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.lstsq.html)\n",
    "    to perform the regression. To regress the dependent\n",
    "    variable `y` on regressors `X`, you need to call `lstsq(X, y)`.\n",
    "    To include the intercept, you manually have to create `X` such that the \n",
    "    first column contains only ones.\n",
    "\n",
    "6.  **[Advanced]** \n",
    "    Plot your results: for each decade, create a scatter plot of the raw \n",
    "    data and overlay it with the regression line you estimated."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FIE463",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
